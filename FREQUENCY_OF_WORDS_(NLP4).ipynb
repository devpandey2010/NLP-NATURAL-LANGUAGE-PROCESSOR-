{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6eiw2zpJF57",
        "outputId": "45cce7f2-d9d9-476d-edb8-580de64acdd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'nlp': 2, 'is': 1, 'amazing': 1, ',': 1, 'makes': 1, 'machines': 1, 'understand': 1, 'language': 1})\n"
          ]
        }
      ],
      "source": [
        "#word frequency distribution\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "def word_frequency(text):\n",
        "  words=word_tokenize(text.lower())\n",
        "  return Counter(words)\n",
        "\n",
        "text=\"NLP is amazing, NLP makes machines understand Language\"\n",
        "print(word_frequency(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BAG of Words"
      ],
      "metadata": {
        "id": "s2neh-7XJzXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus=[\"NLP is fun and amazing\",\n",
        "        \"Machines understand NLP and Text\",\n",
        "        \"Text processing is a part of NLP\"\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "Zs9zI70hJJbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vectorizer=CountVectorizer()\n",
        "x=Vectorizer.fit_transform(corpus)\n",
        "print(Vectorizer.get_feature_names_out())\n",
        "print(x.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5za3LkOmJJeX",
        "outputId": "7449c7af-d1e6-4291-ea18-90ddad54db89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amazing' 'and' 'fun' 'is' 'machines' 'nlp' 'of' 'part' 'processing'\n",
            " 'text' 'understand']\n",
            "[[1 1 1 1 0 1 0 0 0 0 0]\n",
            " [0 1 0 0 1 1 0 0 0 1 1]\n",
            " [0 0 0 1 0 1 1 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "6BpBDtgsLdIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ou calculated:\n",
        "TF = 1/5 = 0.2\n",
        "\n",
        "IDF = log(3/2) ≈ 0.176\n",
        "\n",
        "So, TF × IDF = 0.2 × 0.176 = 0.0352 ❌\n",
        "\n",
        "But the model gave 0.534 — why?\n",
        "\n",
        "I have upload all the details about the math intution behind tf-idf go and check there\n",
        "\n"
      ],
      "metadata": {
        "id": "PElHIvooPOWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer=TfidfVectorizer()\n",
        "y=vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(y.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTCbsZ6GJJkO",
        "outputId": "0dceede0-cf5a-4ae7-eec5-5e943a859fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amazing' 'and' 'fun' 'is' 'machines' 'nlp' 'of' 'part' 'processing'\n",
            " 'text' 'understand']\n",
            "[[0.53409337 0.40619178 0.53409337 0.40619178 0.         0.31544415\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.40619178 0.         0.         0.53409337 0.31544415\n",
            "  0.         0.         0.         0.40619178 0.53409337]\n",
            " [0.         0.         0.         0.35829137 0.         0.27824521\n",
            "  0.4711101  0.4711101  0.4711101  0.35829137 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "tokenize_docs=[docs.lower().split() for docs in corpus]\n",
        "tokenize_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMcJXoztJJnu",
        "outputId": "9ec809a8-8041-4e44-c882-4ee01003de4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['nlp', 'is', 'fun', 'and', 'amazing'],\n",
              " ['machines', 'understand', 'nlp', 'and', 'text'],\n",
              " ['text', 'processing', 'is', 'a', 'part', 'of', 'nlp']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=sorted(set(word for doc in tokenize_docs for word in doc))\n",
        "vocab_index={word:idx for idx,word in enumerate(vocab)}\n",
        "print(vocab)\n",
        "print(vocab_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOb5KbEISb3H",
        "outputId": "42a583f3-de82-4810-dd3d-7d2d8128da16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'amazing', 'and', 'fun', 'is', 'machines', 'nlp', 'of', 'part', 'processing', 'text', 'understand']\n",
            "{'a': 0, 'amazing': 1, 'and': 2, 'fun': 3, 'is': 4, 'machines': 5, 'nlp': 6, 'of': 7, 'part': 8, 'processing': 9, 'text': 10, 'understand': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf(doc_tokens):\n",
        "  tf_vector=np.zeros(len(vocab))\n",
        "  word_counts=Counter(doc_tokens)\n",
        "  for word,count in word_counts.items():\n",
        "    tf_vector[vocab_index[word]]=count/len(doc_tokens)\n",
        "  return tf_vector\n",
        "\n",
        "tf_matrix=np.array([compute_tf(doc) for doc in tokenize_docs])\n",
        "tf_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzholPpwSbLV",
        "outputId": "3db1702d-55ac-4040-f2fd-011238f99283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.2       , 0.2       , 0.2       , 0.2       ,\n",
              "        0.        , 0.2       , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.2       , 0.        , 0.        ,\n",
              "        0.2       , 0.2       , 0.        , 0.        , 0.        ,\n",
              "        0.2       , 0.2       ],\n",
              "       [0.14285714, 0.        , 0.        , 0.        , 0.14285714,\n",
              "        0.        , 0.14285714, 0.14285714, 0.14285714, 0.14285714,\n",
              "        0.14285714, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf():\n",
        "  N=len(tokenize_docs)\n",
        "  idf=np.zeros(len(vocab))\n",
        "  for word,idx in vocab_index.items():\n",
        "    df=sum(1 for doc in tokenize_docs if word in doc)\n",
        "    idf[idx]=math.log((N+1)/(df+1))+1\n",
        "  return idf\n",
        "\n",
        "idf_vector=compute_idf()\n",
        "print(idf_vector)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV2QuhMaSbIC",
        "outputId": "bb5cc264-9f4b-49b8-eb24-1ba036d2f631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.69314718 1.69314718 1.28768207 1.69314718 1.28768207 1.69314718\n",
            " 1.         1.69314718 1.69314718 1.69314718 1.28768207 1.69314718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix=tf_matrix*idf_vector\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4NwOKnNSbE3",
        "outputId": "f0cfa029-cd8a-4087-dd3d-90e12dfa2e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.33862944 0.25753641 0.33862944 0.25753641 0.\n",
            "  0.2        0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.25753641 0.         0.         0.33862944\n",
            "  0.2        0.         0.         0.         0.25753641 0.33862944]\n",
            " [0.24187817 0.         0.         0.         0.18395458 0.\n",
            "  0.14285714 0.24187817 0.24187817 0.24187817 0.18395458 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Task-Feature Extraction\n"
      ],
      "metadata": {
        "id": "6aTWe0qwWGY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sorted according to weightage\n",
        "import numpy as np\n",
        "feature_array=np.array(vectorizer.get_feature_names_out())\n",
        "importance=np.argsort(x.toarray()).flatten()[::-1]  #First always convert into 1d vector\n",
        "keywords=feature_array[importance[:10]]  #top 10 imporatnat feature\n",
        "\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KCoEhwwSa-d",
        "outputId": "bc1cd0e3-8770-4943-d325-5a9f44eb5fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['text' 'processing' 'part' 'nlp' 'of' 'is' 'understand' 'machines' 'fun'\n",
            " 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top=3\n",
        "for i,row in enumerate(tfidf_matrix):\n",
        "  top_indices=row.argsort()[::-1][:top]\n",
        "  keywords=[vocab[idx] for idx in top_indices]\n",
        "  print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8SiVL4ESa7i",
        "outputId": "6c30d969-d6ac-4c98-a363-3f0033a2aaf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fun', 'amazing', 'and']\n",
            "['understand', 'machines', 'and']\n",
            "['part', 'processing', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J2UI0FPwSa4Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}