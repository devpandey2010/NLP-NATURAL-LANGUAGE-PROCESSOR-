{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## WORD COUNT"
      ],
      "metadata": {
        "id": "MuWBF12hBTNq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiADEYi7AesN"
      },
      "outputs": [],
      "source": [
        "#inbuilt python libraries for word count\n",
        "#regex for counter\n",
        "#NLTK for counting words\n",
        "#spacy for word count\n",
        "#counter\n",
        "#NLP tasks- short vs long sentences\n",
        "#NLP tasks-rule based sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text.split()\n",
        "Behind the scenes, Python:\n",
        "\n",
        "Splits on whitespace → means:\n",
        "\n",
        "' ' (space)\n",
        "\n",
        "\\t (tab)\n",
        "\n",
        "\\n (newline)\n",
        "\n",
        "Ignores extra spaces — multiple spaces ek hi delimiter treat hote hain.\n",
        "\n",
        "Removes leading/trailing spaces automatically.\n",
        "\n",
        "text = \"Hello\\tworld\\npython\"\n",
        "print(text.split())\n",
        "\n",
        " .split(' ') vs .split() — Difference:\n",
        "python\n",
        "Copy code\n",
        "text = \"  Hello   world  \"\n",
        "print(text.split())      # Default = all whitespace, ignores extras\n",
        "print(text.split(' '))   # Only space, doesn't ignore extras\n",
        "\n",
        "'Hello', 'world']          ✅ clean\n",
        "['', '', 'Hello', '', '', 'world', '', ''] ❌ extra empty strings\n",
        "\n"
      ],
      "metadata": {
        "id": "NdhVo94wCmDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Natural language processing (NLP) is a fascinating domain\"\n",
        "words=text.split() #It will split all the words\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssj0ldopAjRi",
        "outputId": "e78cbfdd-de2f-4258-b0e2-d71bcf248f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(NLP)',\n",
              " 'is',\n",
              " 'a',\n",
              " 'fascinating',\n",
              " 'domain']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHS5MqQSAjUp",
        "outputId": "a8d3dbc8-3317-4c0b-e284-32c1f37fc1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word count\",len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENsgEp5tAjXx",
        "outputId": "66f67388-caf6-42ab-ba06-8893caee8794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_counts(text):\n",
        "  text=text.split()\n",
        "  count=len(text)\n",
        "  print(\"Word Count:\",count)\n",
        "\n",
        "text=\"Natural language processing (NLP) is a fascinating domain,I love it\"\n",
        "word_counts(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t1TyHIXAja4",
        "outputId": "0cc11822-6f4c-46a1-cecc-1d0fde9bc24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#if we want to split\n",
        "'''space\n",
        "\n",
        ", (comma)\n",
        "\n",
        ". (dot)\n",
        "\n",
        "( ) (brackets)\n",
        "\n",
        "and other punctuations'''\n",
        "\n",
        "solution we can have with regex"
      ],
      "metadata": {
        "id": "fkBG4o1ZEw9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if we want to split\n",
        "'''space\n",
        ", (comma)\n",
        ". (dot)\n",
        "( ) (brackets)\n",
        "and other punctuations'''"
      ],
      "metadata": {
        "id": "LIb36ywJAjeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text=\"Natural language processing (NLP) is a fascinating domain,I love it\"\n",
        "words=re.split(r\"[' ',:.!?;]\",text)\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUJxXchNAjhI",
        "outputId": "2629da1c-f747-4f27-9518-37f009178290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(NLP)', 'is', 'a', 'fascinating', 'domain', 'I', 'love', 'it']\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#regex\n",
        "def count_words(text):\n",
        "  words=re.split(r'\\b\\w+\\b',text)\n",
        "  print(words)\n",
        "  return len(words)\n",
        "\n",
        "text=\"Natural language Processing\"\n",
        "count_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOfjW66vAjke",
        "outputId": "ead4fe50-7514-42bd-aac3-05a02946adbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', ' ', ' ', '']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK-Python (Natural Language Toolkit)\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp9w0mC0Ajnm",
        "outputId": "d6cc39a2-642a-4679-c316-bbb694a1a85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize # this is for word count\n",
        "from nltk import sent_tokenize # this is for sentence count\n",
        "def count_words_nltk(text):\n",
        "  words=sent_tokenize(text)\n",
        "  print(words)\n",
        "  return len(words)\n",
        "\n",
        "text='''Let's Explore NLP with python.\\n We will be using NLTK.\\n we might also use spacy./n'''\n",
        "count_words_nltk(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jl9cPwnAjrG",
        "outputId": "f104ca92-1cf0-4185-cb91-6029197625f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's Explore NLP with python.\", 'We will be using NLTK.', 'we might also use spacy./n']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy implementation\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def count_words_spacy(text):\n",
        "  doc=nlp(text)\n",
        "  words=[token.text for token in doc if not token.is_punct]\n",
        "  print(words)\n",
        "  return len(words)\n",
        "\n",
        "text=\"Let's Explore NLP with python .\"\n",
        "print(count_words_spacy(text))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W30WpAG2AjuO",
        "outputId": "a83feeea-ed53-4f75-d77e-e2969eb74567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'s\", 'Explore', 'NLP', 'with', 'python']\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Frequency\n",
        "from collections import Counter\n",
        "\n",
        "def word_frequency(text):\n",
        "  doc=nlp(text)\n",
        "  words=[token.text.lower() for token in doc]\n",
        "  return Counter(words)\n",
        "\n",
        "text=\"NLP is fun, NLP is powerful,NLP is amzing in future .\"\n",
        "\n",
        "print(word_frequency(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3vDGv7mAjxZ",
        "outputId": "cbf287ca-c45a-43dd-e672-6e87407eac33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'nlp': 3, 'is': 3, ',': 2, 'fun': 1, 'powerful': 1, 'amzing': 1, 'in': 1, 'future': 1, '.': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def word_frequency(text):\n",
        "  doc=nlp(text)\n",
        "  words=[token.text.lower() for token in doc if not token.is_punct]\n",
        "  return Counter(words)\n",
        "\n",
        "text=\"NLP is fun, NLP is powerful,NLP is amzing in future .\"\n",
        "\n",
        "print(word_frequency(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56kROj6FAj0o",
        "outputId": "d28128ab-e041-48b6-dc0f-116914f10cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'nlp': 3, 'is': 3, 'fun': 1, 'powerful': 1, 'amzing': 1, 'in': 1, 'future': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Task 1"
      ],
      "metadata": {
        "id": "UXhUYmlRc-0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#short vs long sentences\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def category_sentences(text):\n",
        "  sentences=sent_tokenize(text)\n",
        "  categorize={\"short\":[],\"long\":[]}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    doc=nlp(sentence)\n",
        "    word_count=len([token.text for token in doc if not token.is_punct])\n",
        "    if word_count < 5:\n",
        "      categorize[\"short\"].append(sentence)\n",
        "    else:\n",
        "      categorize[\"long\"].append(sentence)\n",
        "\n",
        "  return categorize\n",
        "\n",
        "text=\"Hello,NlP is fun. I like it very much. i hope everyone is enjoying it\"\n",
        "category_sentences(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KZssNGLAj6x",
        "outputId": "8d123a4b-43a9-4b86-8e4f-8c1beba5b28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'short': ['Hello,NlP is fun.'],\n",
              " 'long': ['I like it very much.', 'i hope everyone is enjoying it']}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Question_category(text):\n",
        "  sentences=sent_tokenize(text)\n",
        "  categorize={\"Question\":[],\"Not a Question\":[]}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    doc=nlp(sentence)\n",
        "    word_count=[token.text for token in doc]\n",
        "    if '?' in word_count:\n",
        "      categorize[\"Question\"].append(sentence)\n",
        "    else:\n",
        "      categorize[\"Not a Question\"].append(sentence)\n",
        "\n",
        "  return categorize\n",
        "\n",
        "text=\"What is Your name ? My name is Dev pandey. HOw are You ? I am Fine\"\n",
        "Question_category(text)  #spaces are very important"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpiXEEUWAj-S",
        "outputId": "2e00258d-e8ae-403e-e82e-0e42ebbd81da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Question': ['What is Your name ?', 'HOw are You ?'],\n",
              " 'Not a Question': ['My name is Dev pandey.', 'I am Fine']}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis"
      ],
      "metadata": {
        "id": "mg1qPhe7kFlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rule based sentiment analysis\n",
        "\n",
        "import time\n",
        "\n",
        "positive_words={\"happy\",\"Delight\",\"Moody\",\"Cheerful\"}\n",
        "negative_words={\"sad\",\"lonliness\",\"dark\"}"
      ],
      "metadata": {
        "id": "6Q4n5Fw6jSIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_analyzer(text):\n",
        "  start_time=time.time()\n",
        "  words=text.lower().split()\n",
        "\n",
        "  pos_count=sum(1 for word in words if word in positive_words)\n",
        "  neg_count=sum(1 for word in words if word in negative_words)\n",
        "\n",
        "  sentiment=\"Positive Sentiment\" if pos_count > neg_count else \"Negative Sentiment\" if neg_count>pos_count else \"Neutral statement\"\n",
        "\n",
        "  end_time=time.time()\n",
        "  execution_time=end_time-start_time\n",
        "\n",
        "  return sentiment,execution_time\n",
        "\n"
      ],
      "metadata": {
        "id": "kNEEeK96jSLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='I am very Happy'\n",
        "sentiment,execution_time=sentiment_analyzer(text)\n",
        "print(\"Sentiment:\",sentiment,\"Execution_time:\",execution_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yob40mejSPI",
        "outputId": "3e38566a-2e93-4af9-bd88-7831b9abe301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Positive Sentiment Execution_time: 1.0967254638671875e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='I am very SAd'\n",
        "sentiment,execution_time=sentiment_analyzer(text)\n",
        "print(\"Sentiment:\",sentiment,\"Execution_time:\",execution_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p-dJJw7jSSU",
        "outputId": "d234a04a-1357-41a3-8de2-5975d720aaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Negative Sentiment Execution_time: 1.2159347534179688e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='I am very'\n",
        "sentiment,execution_time=sentiment_analyzer(text)\n",
        "print(\"Sentiment:\",sentiment,\"Execution_time:\",execution_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NilgbT_p_KH",
        "outputId": "ad022636-0147-45b5-b094-5872e010a4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Neutral statement Execution_time: 7.62939453125e-06\n"
          ]
        }
      ]
    }
  ]
}